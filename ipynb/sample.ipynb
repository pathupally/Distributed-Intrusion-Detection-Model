{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.typing import NDArray\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/UNSW_NB15_training-set.csv') # Load dataset\n",
    "data = data[0:100000] # Limit to 100k rows for faster processing\n",
    "data.to_csv('s.csv')\n",
    "categorical_columns = ['proto', 'service', 'state'] # Categorical columns to be converted to numerical\\\n",
    "data = pd.get_dummies(data, columns=categorical_columns) # Convert categorical columns to numerical\n",
    "labels = data['label'].values # Extract labels\n",
    "features = data.drop(columns=['label', 'attack_cat'], axis=1) # Extract features\n",
    "\n",
    "\n",
    "''' Training and Testing Data '''\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.2, random_state=42, stratify=labels) # Split data into training and testing sets\n",
    "scaler = StandardScaler() # Initialize scaler\n",
    "X_train = scaler.fit_transform(X_train) # Fit and transform training data\n",
    "X_test = scaler.transform(X_test) # Transform testing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 6. Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor  = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "# 7. Create TensorDataset for convenient data handling\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset  = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# 8. Use DataLoader to batch and shuffle the data\n",
    "batch_size = 64  # industry-standard batch sizes are powers of 2 (32, 64, 128, etc.)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=195, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 9. Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        # Define layers:\n",
    "        self.fc1 = nn.Linear(input_size, 64)   # fully connected layer 1: input -> 64 hidden units\n",
    "        self.fc2 = nn.Linear(64, 1)           # fully connected layer 2: 64 -> 1 output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass (how data moves through the network)\n",
    "        x = F.relu(self.fc1(x))   # apply ReLU activation after first layer\n",
    "        x = self.fc2(x)           # second layer (output logits)\n",
    "        return x\n",
    "\n",
    "# 10. Initialize the model\n",
    "input_dim = X_train_tensor.shape[1]   # number of features after encoding\n",
    "model = Net(input_dim)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=195, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.1743\n",
      "Epoch [2/100], Loss: 0.1275\n",
      "Epoch [3/100], Loss: 0.1047\n",
      "Epoch [4/100], Loss: 0.0915\n",
      "Epoch [5/100], Loss: 0.0833\n",
      "Epoch [6/100], Loss: 0.0769\n",
      "Epoch [7/100], Loss: 0.0722\n",
      "Epoch [8/100], Loss: 0.0678\n",
      "Epoch [9/100], Loss: 0.0647\n",
      "Epoch [10/100], Loss: 0.0623\n",
      "Epoch [11/100], Loss: 0.0603\n",
      "Epoch [12/100], Loss: 0.0589\n",
      "Epoch [13/100], Loss: 0.0576\n",
      "Epoch [14/100], Loss: 0.0566\n",
      "Epoch [15/100], Loss: 0.0556\n",
      "Epoch [16/100], Loss: 0.0550\n",
      "Epoch [17/100], Loss: 0.0540\n",
      "Epoch [18/100], Loss: 0.0535\n",
      "Epoch [19/100], Loss: 0.0527\n",
      "Epoch [20/100], Loss: 0.0520\n",
      "Epoch [21/100], Loss: 0.0518\n",
      "Epoch [22/100], Loss: 0.0511\n",
      "Epoch [23/100], Loss: 0.0503\n",
      "Epoch [24/100], Loss: 0.0502\n",
      "Epoch [25/100], Loss: 0.0495\n",
      "Epoch [26/100], Loss: 0.0499\n",
      "Epoch [27/100], Loss: 0.0487\n",
      "Epoch [28/100], Loss: 0.0484\n",
      "Epoch [29/100], Loss: 0.0479\n",
      "Epoch [30/100], Loss: 0.0480\n",
      "Epoch [31/100], Loss: 0.0479\n",
      "Epoch [32/100], Loss: 0.0472\n",
      "Epoch [33/100], Loss: 0.0469\n",
      "Epoch [34/100], Loss: 0.0473\n",
      "Epoch [35/100], Loss: 0.0457\n",
      "Epoch [36/100], Loss: 0.0465\n",
      "Epoch [37/100], Loss: 0.0461\n",
      "Epoch [38/100], Loss: 0.0459\n",
      "Epoch [39/100], Loss: 0.0456\n",
      "Epoch [40/100], Loss: 0.0456\n",
      "Epoch [41/100], Loss: 0.0452\n",
      "Epoch [42/100], Loss: 0.0443\n",
      "Epoch [43/100], Loss: 0.0451\n",
      "Epoch [44/100], Loss: 0.0442\n",
      "Epoch [45/100], Loss: 0.0438\n",
      "Epoch [46/100], Loss: 0.0446\n",
      "Epoch [47/100], Loss: 0.0441\n",
      "Epoch [48/100], Loss: 0.0442\n",
      "Epoch [49/100], Loss: 0.0436\n",
      "Epoch [50/100], Loss: 0.0428\n",
      "Epoch [51/100], Loss: 0.0438\n",
      "Epoch [52/100], Loss: 0.0430\n",
      "Epoch [53/100], Loss: 0.0433\n",
      "Epoch [54/100], Loss: 0.0430\n",
      "Epoch [55/100], Loss: 0.0430\n",
      "Epoch [56/100], Loss: 0.0427\n",
      "Epoch [57/100], Loss: 0.0421\n",
      "Epoch [58/100], Loss: 0.0431\n",
      "Epoch [59/100], Loss: 0.0418\n",
      "Epoch [60/100], Loss: 0.0428\n",
      "Epoch [61/100], Loss: 0.0418\n",
      "Epoch [62/100], Loss: 0.0427\n",
      "Epoch [63/100], Loss: 0.0414\n",
      "Epoch [64/100], Loss: 0.0419\n",
      "Epoch [65/100], Loss: 0.0428\n",
      "Epoch [66/100], Loss: 0.0420\n",
      "Epoch [67/100], Loss: 0.0416\n",
      "Epoch [68/100], Loss: 0.0417\n",
      "Epoch [69/100], Loss: 0.0410\n",
      "Epoch [70/100], Loss: 0.0407\n",
      "Epoch [71/100], Loss: 0.0420\n",
      "Epoch [72/100], Loss: 0.0409\n",
      "Epoch [73/100], Loss: 0.0409\n",
      "Epoch [74/100], Loss: 0.0405\n",
      "Epoch [75/100], Loss: 0.0408\n",
      "Epoch [76/100], Loss: 0.0403\n",
      "Epoch [77/100], Loss: 0.0403\n",
      "Epoch [78/100], Loss: 0.0408\n",
      "Epoch [79/100], Loss: 0.0406\n",
      "Epoch [80/100], Loss: 0.0400\n",
      "Epoch [81/100], Loss: 0.0405\n",
      "Epoch [82/100], Loss: 0.0397\n",
      "Epoch [83/100], Loss: 0.0397\n",
      "Epoch [84/100], Loss: 0.0399\n",
      "Epoch [85/100], Loss: 0.0396\n",
      "Epoch [86/100], Loss: 0.0399\n",
      "Epoch [87/100], Loss: 0.0403\n",
      "Epoch [88/100], Loss: 0.0395\n",
      "Epoch [89/100], Loss: 0.0397\n",
      "Epoch [90/100], Loss: 0.0395\n",
      "Epoch [91/100], Loss: 0.0393\n",
      "Epoch [92/100], Loss: 0.0390\n",
      "Epoch [93/100], Loss: 0.0400\n",
      "Epoch [94/100], Loss: 0.0388\n",
      "Epoch [95/100], Loss: 0.0390\n",
      "Epoch [96/100], Loss: 0.0397\n",
      "Epoch [97/100], Loss: 0.0386\n",
      "Epoch [98/100], Loss: 0.0388\n",
      "Epoch [99/100], Loss: 0.0398\n",
      "Epoch [100/100], Loss: 0.0388\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for batch_X, batch_Y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_Y = batch_Y.to(device)\n",
    "        \n",
    "        outputs = model(batch_X)\n",
    "        outputs = outputs.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, batch_Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        \n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=195, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.00%\n",
      "Precision: 98.14%\n",
      "Recall: 97.68%\n",
      "F1 Score: 97.91%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "TP = FP = FN = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_Y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_Y = batch_Y.to(device)\n",
    "        \n",
    "        outputs = model(batch_X)\n",
    "        \n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs >= 0.5).float()\n",
    "                # Flatten predictions and labels to 1D\n",
    "        preds = preds.view(-1)\n",
    "        labels = batch_Y.view(-1)\n",
    "        # Count correct predictions\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total   += labels.size(0)\n",
    "        \n",
    "        TP += ((preds == 1) & (labels == 1)).sum().item()\n",
    "        FP += ((preds == 1) & (labels == 0)).sum().item()\n",
    "        FN += ((preds == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.2f}%\")\n",
    "print(f\"Recall: {recall*100:.2f}%\")\n",
    "print(f\"F1 Score: {f1_score*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
